@article{bias,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {115},
numpages = {35},
keywords = {Fairness and bias in artificial intelligence, deep learning, machine learning, natural language processing, representation learning}
}

@article{spo,
author = {Elmachtoub, Adam N. and Grigas, Paul},
title = {Smart “Predict, then Optimize”},
journal = {Management Science},
volume = {68},
number = {1},
pages = {9-26},
year = {2022},
doi = {10.1287/mnsc.2020.3922},

        URL = {https://doi.org/10.1287/mnsc.2020.3922},
        eprint = {https://doi.org/10.1287/mnsc.2020.3922},
        abstract = {
        Many real-world analytics problems involve two significant challenges: prediction and optimization. 
        Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize.
         By and large, machine learning tools are intended to minimize prediction error and do not account for how 
         the predictions will be used in the downstream optimization problem. In contrast, we propose a new and 
         very general framework, called Smart “Predict, then Optimize” (SPO), which directly leverages the 
         optimization problem structure—that is, its objective and constraints—for designing better prediction 
         models. A key component of our framework is the SPO loss function, which measures the decision error 
         induced by a prediction. Training a prediction model with respect to the SPO loss is computationally 
         challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call 
         the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to 
         the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, 
         or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path 
         and portfolio-optimization problems show that the SPO framework can lead to significant improvement under 
         the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. 
         We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the 
         ground truth is highly nonlinear.This paper was accepted by Yinyu Ye, optimization.Supplemental Material: 
         Data and the online appendix are available at https://doi.org/10.1287/mnsc.2020.3922 
        }
}

@article{ml_policy,
  title={Explainable machine learning for public policy: Use cases, gaps, and research directions},
  volume={5},
  DOI={10.1017/dap.2023.2},
  journal={Data \& Policy},
  author={Amarasinghe, Kasun and Rodolfa, Kit T. and Lamba, Hemank and Ghani, Rayid},
  year={2023},
  pages={e5}
}


@article{Mandi_2024,
   title={Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities},
   volume={80},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1.15320},
   DOI={10.1613/jair.1.15320},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Mandi, Jayanta and Kotary, James and Berden, Senne and Mulamba, Maxime and Bucarey, Victor and Guns, Tias and Fioretto, Ferdinando},
   year={2024},
   month=aug, pages={1623–1701} }

@inproceedings{Google_DFL,
        title={Case Study: Applying Decision Focused Learning in the Real World},
        author	= {Aditya Mayte and Aparna Taneja and Kai Wang and Milind Shashikant Tambe and Shresth Verma},
        year	= {2022}
        }

@article{P.Whittle,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3214163},
 abstract = {We consider a population of n projects which in general continue to evolve whether in operation or not (although by different rules). It is desired to choose the projects in operation at each instant of time so as to maximise the expected rate of reward, under a constraint upon the expected number of projects in operation. The Lagrange multiplier associated with this constraint defines an index which reduces to the Gittins index when projects not being operated are static. If one is constrained to operate m projects exactly then arguments are advanced to support the conjecture that, for m and n large in constant ratio, the policy of operating the m projects of largest current index is nearly optimal. The index is evaluated for some particular projects.},
 author = {P. Whittle},
 journal = {Journal of Applied Probability},
 pages = {287--298},
 publisher = {Applied Probability Trust},
 title = {Restless Bandits: Activity Allocation in a Changing World},
 urldate = {2024-12-12},
 volume = {25},
 year = {1988}
}

@article{SR_2024,
	title = {Interpretable scientific discovery with symbolic regression: a review},
	volume = {57},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-023-10622-0},
	doi = {10.1007/s10462-023-10622-0},
	abstract = {Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery tool, achieving significant advances in various application domains ranging from fundamental to applied sciences. In this survey, we present a structured and comprehensive overview of symbolic regression methods, review the adoption of these methods for model discovery in various areas, and assess their effectiveness. We have also grouped state-of-the-art symbolic regression applications in a categorized manner in a living review.},
	number = {1},
	journal = {Artificial Intelligence Review},
	author = {Makke, Nour and Chawla, Sanjay},
	month = jan,
	year = {2024},
	pages = {2},
}

@misc{kans_2024,
      title={KAN: Kolmogorov-Arnold Networks}, 
      author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljačić and Thomas Y. Hou and Max Tegmark},
      year={2024},
      eprint={2404.19756},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.19756}, 
}

@misc{yu2024kanmlpfairercomparison,
      title={KAN or MLP: A Fairer Comparison}, 
      author={Runpeng Yu and Weihao Yu and Xinchao Wang},
      year={2024},
      eprint={2407.16674},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.16674}, 
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Springer Nature},
  doi={10.1038/323533a0},
  url={https://www.nature.com/articles/323533a0}
}

@Inbook{Kruse2022,
author="Kruse, Rudolf
and Mostaghim, Sanaz
and Borgelt, Christian
and Braune, Christian
and Steinbrecher, Matthias",
title="Introduction to Evolutionary Algorithms",
bookTitle="Computational Intelligence: A Methodological Introduction",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="225--254",
abstract="Evolutionary algorithmEvolutionary algorithms comprise a class of optimization techniques that imitate principles of biological evolution. They belong to the family of metaheuristicsMetaheuristics, which also includes, for example, particle swarm and ant colony optimization, which are inspired by other biological structures and processes, as well as classical methods like simulated annealing, which is inspired by a thermodynamical process. The core principle of evolutionary algorithms is to apply evolution principles like mutation and selection to populations of candidate solutions in order to find a (sufficiently good) solution for a given optimization problem.",
isbn="978-3-030-42227-1",
doi="10.1007/978-3-030-42227-1_11",
url="https://doi.org/10.1007/978-3-030-42227-1_11"
}


@article{masoudnia_mixture_2014,
	title = {Mixture of experts: a literature survey},
	volume = {42},
	issn = {1573-7462},
	shorttitle = {Mixture of experts},
	url = {https://doi.org/10.1007/s10462-012-9338-y},
	doi = {10.1007/s10462-012-9338-y},
	abstract = {Mixture of experts (ME) is one of the most popular and interesting combining methods, which has great potential to improve performance in machine learning. ME is established based on the divide-and-conquer principle in which the problem space is divided between a few neural network experts, supervised by a gating network. In earlier works on ME, different strategies were developed to divide the problem space between the experts. To survey and analyse these methods more clearly, we present a categorisation of the ME literature based on this difference. Various ME implementations were classified into two groups, according to the partitioning strategies used and both how and when the gating network is involved in the partitioning and combining procedures. In the first group, The conventional ME and the extensions of this method stochastically partition the problem space into a number of subspaces using a special employed error function, and experts become specialised in each subspace. In the second group, the problem space is explicitly partitioned by the clustering method before the experts’ training process starts, and each expert is then assigned to one of these sub-spaces. Based on the implicit problem space partitioning using a tacit competitive process between the experts, we call the first group the mixture of implicitly localised experts (MILE), and the second group is called mixture of explicitly localised experts (MELE), as it uses pre-specified clusters. The properties of both groups are investigated in comparison with each other. Investigation of MILE versus MELE, discussing the advantages and disadvantages of each group, showed that the two approaches have complementary features. Moreover, the features of the ME method are compared with other popular combining methods, including boosting and negative correlation learning methods. As the investigated methods have complementary strengths and limitations, previous researches that attempted to combine their features in integrated approaches are reviewed and, moreover, some suggestions are proposed for future research directions.},
	language = {en},
	number = {2},
	urldate = {2024-11-28},
	journal = {Artificial Intelligence Review},
	author = {Masoudnia, Saeed and Ebrahimpour, Reza},
	month = aug,
	year = {2014},
	keywords = {Artificial Intelligence, Classifier combining, Mixture of experts, Mixture of explicitly localised expert, Mixture of implicitly localised experts},
	pages = {275--293},
	file = {Full Text PDF:files/1084/Masoudnia and Ebrahimpour - 2014 - Mixture of experts a literature survey.pdf:application/pdf},
}

@misc{test_functions,
author = "{Wikipedia contributors}",
title = "Test functions for optimization --- {Wikipedia}{,} The Free Encyclopedia",
year = "2024",
url = "https://en.wikipedia.org/w/index.php?title=Test_functions_for_optimization&oldid=1262242809",
note = "[Online; accessed 13-December-2024]"
}




