@article{spo,
author = {Elmachtoub, Adam N. and Grigas, Paul},
title = {Smart “Predict, then Optimize”},
journal = {Management Science},
volume = {68},
number = {1},
pages = {9-26},
year = {2022},
doi = {10.1287/mnsc.2020.3922},

        URL = {https://doi.org/10.1287/mnsc.2020.3922},
        eprint = {https://doi.org/10.1287/mnsc.2020.3922},
        abstract = {
        Many real-world analytics problems involve two significant challenges: prediction and optimization. 
        Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize.
         By and large, machine learning tools are intended to minimize prediction error and do not account for how 
         the predictions will be used in the downstream optimization problem. In contrast, we propose a new and 
         very general framework, called Smart “Predict, then Optimize” (SPO), which directly leverages the 
         optimization problem structure—that is, its objective and constraints—for designing better prediction 
         models. A key component of our framework is the SPO loss function, which measures the decision error 
         induced by a prediction. Training a prediction model with respect to the SPO loss is computationally 
         challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call 
         the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to 
         the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, 
         or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path 
         and portfolio-optimization problems show that the SPO framework can lead to significant improvement under 
         the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. 
         We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the 
         ground truth is highly nonlinear.This paper was accepted by Yinyu Ye, optimization.Supplemental Material: 
         Data and the online appendix are available at https://doi.org/10.1287/mnsc.2020.3922 
        }
}

@article{Mandi_2024,
   title={Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities},
   volume={80},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1.15320},
   DOI={10.1613/jair.1.15320},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Mandi, Jayanta and Kotary, James and Berden, Senne and Mulamba, Maxime and Bucarey, Victor and Guns, Tias and Fioretto, Ferdinando},
   year={2024},
   month=aug, pages={1623–1701} }

@inproceedings{Google_DFL,
        title={Case Study: Applying Decision Focused Learning in the Real World},
        author	= {Aditya Mayte and Aparna Taneja and Kai Wang and Milind Shashikant Tambe and Shresth Verma},
        year	= {2022}
        }

@article{P.Whittle,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3214163},
 abstract = {We consider a population of n projects which in general continue to evolve whether in operation or not (although by different rules). It is desired to choose the projects in operation at each instant of time so as to maximise the expected rate of reward, under a constraint upon the expected number of projects in operation. The Lagrange multiplier associated with this constraint defines an index which reduces to the Gittins index when projects not being operated are static. If one is constrained to operate m projects exactly then arguments are advanced to support the conjecture that, for m and n large in constant ratio, the policy of operating the m projects of largest current index is nearly optimal. The index is evaluated for some particular projects.},
 author = {P. Whittle},
 journal = {Journal of Applied Probability},
 pages = {287--298},
 publisher = {Applied Probability Trust},
 title = {Restless Bandits: Activity Allocation in a Changing World},
 urldate = {2024-12-12},
 volume = {25},
 year = {1988}
}

@article{SR_2024,
	title = {Interpretable scientific discovery with symbolic regression: a review},
	volume = {57},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-023-10622-0},
	doi = {10.1007/s10462-023-10622-0},
	abstract = {Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery tool, achieving significant advances in various application domains ranging from fundamental to applied sciences. In this survey, we present a structured and comprehensive overview of symbolic regression methods, review the adoption of these methods for model discovery in various areas, and assess their effectiveness. We have also grouped state-of-the-art symbolic regression applications in a categorized manner in a living review.},
	number = {1},
	journal = {Artificial Intelligence Review},
	author = {Makke, Nour and Chawla, Sanjay},
	month = jan,
	year = {2024},
	pages = {2},
}

@misc{kans_2024,
      title={KAN: Kolmogorov-Arnold Networks}, 
      author={Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Soljačić and Thomas Y. Hou and Max Tegmark},
      year={2024},
      eprint={2404.19756},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.19756}, 
}


