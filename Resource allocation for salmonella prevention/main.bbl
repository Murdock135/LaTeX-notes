% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{spo}{article}{}
      \name{author}{2}{}{%
        {{hash=72760cf91be43b7e1173e52ad2ec220a}{%
           family={Elmachtoub},
           familyi={E\bibinitperiod},
           given={Adam\bibnamedelima N.},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=16bb9e78aff555f22b13c959b6e5fbe4}{%
           family={Grigas},
           familyi={G\bibinitperiod},
           given={Paul},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{5bfb2026397a29247bcd040100a45599}
      \strng{fullhash}{5bfb2026397a29247bcd040100a45599}
      \strng{bibnamehash}{5bfb2026397a29247bcd040100a45599}
      \strng{authorbibnamehash}{5bfb2026397a29247bcd040100a45599}
      \strng{authornamehash}{5bfb2026397a29247bcd040100a45599}
      \strng{authorfullhash}{5bfb2026397a29247bcd040100a45599}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Many real-world analytics problems involve two significant challenges: prediction and optimization. Because of the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart “Predict, then Optimize” (SPO), which directly leverages the optimization problem structure—that is, its objective and constraints—for designing better prediction models. A key component of our framework is the SPO loss function, which measures the decision error induced by a prediction. Training a prediction model with respect to the SPO loss is computationally challenging, and, thus, we derive, using duality theory, a convex surrogate loss function, which we call the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest-path and portfolio-optimization problems show that the SPO framework can lead to significant improvement under the predict-then-optimize paradigm, in particular, when the prediction model being trained is misspecified. We find that linear models trained using SPO+ loss tend to dominate random-forest algorithms, even when the ground truth is highly nonlinear.This paper was accepted by Yinyu Ye, optimization.Supplemental Material: Data and the online appendix are available at https://doi.org/10.1287/mnsc.2020.3922}
      \field{journaltitle}{Management Science}
      \field{number}{1}
      \field{title}{Smart “Predict, then Optimize”}
      \field{volume}{68}
      \field{year}{2022}
      \field{pages}{9\bibrangedash 26}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1287/mnsc.2020.3922
      \endverb
      \verb{eprint}
      \verb https://doi.org/10.1287/mnsc.2020.3922
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1287/mnsc.2020.3922
      \endverb
      \verb{url}
      \verb https://doi.org/10.1287/mnsc.2020.3922
      \endverb
    \endentry
    \entry{Kruse2022}{inbook}{}
      \name{author}{5}{}{%
        {{hash=6d0bb5cbdcc65181905aeeb8d933fbd6}{%
           family={Kruse},
           familyi={K\bibinitperiod},
           given={Rudolf},
           giveni={R\bibinitperiod}}}%
        {{hash=0791bc0e37ea2dfc81bfead0967176ae}{%
           family={Mostaghim},
           familyi={M\bibinitperiod},
           given={Sanaz},
           giveni={S\bibinitperiod}}}%
        {{hash=a1847af399fccad4073edaecefeff149}{%
           family={Borgelt},
           familyi={B\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=41f645b8e4625f6dc51d40c3a9e1b22a}{%
           family={Braune},
           familyi={B\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=b2594ef659e2c532184919e6f10b9511}{%
           family={Steinbrecher},
           familyi={S\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{67dfad1a4ead9fef1fa51b97147dbf14}
      \strng{fullhash}{5fc2ef3c665516ab481aa33badceba92}
      \strng{bibnamehash}{67dfad1a4ead9fef1fa51b97147dbf14}
      \strng{authorbibnamehash}{67dfad1a4ead9fef1fa51b97147dbf14}
      \strng{authornamehash}{67dfad1a4ead9fef1fa51b97147dbf14}
      \strng{authorfullhash}{5fc2ef3c665516ab481aa33badceba92}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Evolutionary algorithmEvolutionary algorithms comprise a class of optimization techniques that imitate principles of biological evolution. They belong to the family of metaheuristicsMetaheuristics, which also includes, for example, particle swarm and ant colony optimization, which are inspired by other biological structures and processes, as well as classical methods like simulated annealing, which is inspired by a thermodynamical process. The core principle of evolutionary algorithms is to apply evolution principles like mutation and selection to populations of candidate solutions in order to find a (sufficiently good) solution for a given optimization problem.}
      \field{booktitle}{Computational Intelligence: A Methodological Introduction}
      \field{isbn}{978-3-030-42227-1}
      \field{title}{Introduction to Evolutionary Algorithms}
      \field{year}{2022}
      \field{pages}{225\bibrangedash 254}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1007/978-3-030-42227-1_11
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-3-030-42227-1_11
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-3-030-42227-1_11
      \endverb
    \endentry
    \entry{kans_2024}{misc}{}
      \name{author}{8}{}{%
        {{hash=1c58afd2046daf734847b96dab2bf564}{%
           family={Liu},
           familyi={L\bibinitperiod},
           given={Ziming},
           giveni={Z\bibinitperiod}}}%
        {{hash=45f61f91eed498475eb6258bbca165c2}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Yixuan},
           giveni={Y\bibinitperiod}}}%
        {{hash=0ab10eff5de236f1519d767e744e5cf0}{%
           family={Vaidya},
           familyi={V\bibinitperiod},
           given={Sachin},
           giveni={S\bibinitperiod}}}%
        {{hash=0875ba21d3e8d07361aecb3962665cff}{%
           family={Ruehle},
           familyi={R\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod}}}%
        {{hash=1a8ee0d401768d2087f11aa88386ce6f}{%
           family={Halverson},
           familyi={H\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=d4bb6bd2724f026f198fb2fddbe7a5f7}{%
           family={Soljačić},
           familyi={S\bibinitperiod},
           given={Marin},
           giveni={M\bibinitperiod}}}%
        {{hash=627dcbbcdb723ced5f9dc3a51fde769d}{%
           family={Hou},
           familyi={H\bibinitperiod},
           given={Thomas\bibnamedelima Y.},
           giveni={T\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
        {{hash=b9b3c445e1013a97fc6318e06799b37b}{%
           family={Tegmark},
           familyi={T\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{4e02a2c1409094c9140c4b77dd060333}
      \strng{fullhash}{fe2782c5c026601c4d592a079123770f}
      \strng{bibnamehash}{4e02a2c1409094c9140c4b77dd060333}
      \strng{authorbibnamehash}{4e02a2c1409094c9140c4b77dd060333}
      \strng{authornamehash}{4e02a2c1409094c9140c4b77dd060333}
      \strng{authorfullhash}{fe2782c5c026601c4d592a079123770f}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{title}{KAN: Kolmogorov-Arnold Networks}
      \field{year}{2024}
      \verb{eprint}
      \verb 2404.19756
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/2404.19756
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/2404.19756
      \endverb
    \endentry
    \entry{SR_2024}{article}{}
      \name{author}{2}{}{%
        {{hash=2d403d51e733236c6bde2075b1613e3f}{%
           family={Makke},
           familyi={M\bibinitperiod},
           given={Nour},
           giveni={N\bibinitperiod}}}%
        {{hash=54ff2f270b001c9403fc0563fe52d8aa}{%
           family={Chawla},
           familyi={C\bibinitperiod},
           given={Sanjay},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{a7f157d3d7b7d9b1c66fbf91834f8e76}
      \strng{fullhash}{a7f157d3d7b7d9b1c66fbf91834f8e76}
      \strng{bibnamehash}{a7f157d3d7b7d9b1c66fbf91834f8e76}
      \strng{authorbibnamehash}{a7f157d3d7b7d9b1c66fbf91834f8e76}
      \strng{authornamehash}{a7f157d3d7b7d9b1c66fbf91834f8e76}
      \strng{authorfullhash}{a7f157d3d7b7d9b1c66fbf91834f8e76}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery tool, achieving significant advances in various application domains ranging from fundamental to applied sciences. In this survey, we present a structured and comprehensive overview of symbolic regression methods, review the adoption of these methods for model discovery in various areas, and assess their effectiveness. We have also grouped state-of-the-art symbolic regression applications in a categorized manner in a living review.}
      \field{issn}{1573-7462}
      \field{journaltitle}{Artificial Intelligence Review}
      \field{month}{1}
      \field{number}{1}
      \field{title}{Interpretable scientific discovery with symbolic regression: a review}
      \field{volume}{57}
      \field{year}{2024}
      \field{pages}{2}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1007/s10462-023-10622-0
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10462-023-10622-0
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10462-023-10622-0
      \endverb
    \endentry
    \entry{Mandi_2024}{article}{}
      \name{author}{7}{}{%
        {{hash=585cbabbe51e265ae378d05f9ca866ac}{%
           family={Mandi},
           familyi={M\bibinitperiod},
           given={Jayanta},
           giveni={J\bibinitperiod}}}%
        {{hash=1e5be91f028b4a84198991171b3c22ea}{%
           family={Kotary},
           familyi={K\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=a1646d5239c2e234838dd07e1656d41c}{%
           family={Berden},
           familyi={B\bibinitperiod},
           given={Senne},
           giveni={S\bibinitperiod}}}%
        {{hash=d0364ca58c188eeea2a901261aa68690}{%
           family={Mulamba},
           familyi={M\bibinitperiod},
           given={Maxime},
           giveni={M\bibinitperiod}}}%
        {{hash=3044e9941e7e88e7099111067c684961}{%
           family={Bucarey},
           familyi={B\bibinitperiod},
           given={Victor},
           giveni={V\bibinitperiod}}}%
        {{hash=2a9ad3a0307f4d7e4e3a864aa91a28ef}{%
           family={Guns},
           familyi={G\bibinitperiod},
           given={Tias},
           giveni={T\bibinitperiod}}}%
        {{hash=d8094523ec4b781111936d02a59b093c}{%
           family={Fioretto},
           familyi={F\bibinitperiod},
           given={Ferdinando},
           giveni={F\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {AI Access Foundation}%
      }
      \strng{namehash}{d92d458f62289bc759d9aaea8a3e46ad}
      \strng{fullhash}{3645d444144521e485546644da6771f2}
      \strng{bibnamehash}{d92d458f62289bc759d9aaea8a3e46ad}
      \strng{authorbibnamehash}{d92d458f62289bc759d9aaea8a3e46ad}
      \strng{authornamehash}{d92d458f62289bc759d9aaea8a3e46ad}
      \strng{authorfullhash}{3645d444144521e485546644da6771f2}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{1076-9757}
      \field{journaltitle}{Journal of Artificial Intelligence Research}
      \field{month}{8}
      \field{title}{Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities}
      \field{volume}{80}
      \field{year}{2024}
      \field{pages}{1623\bibrangedash 1701}
      \range{pages}{79}
      \verb{doi}
      \verb 10.1613/jair.1.15320
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1613/jair.1.15320
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1613/jair.1.15320
      \endverb
    \endentry
    \entry{masoudnia_mixture_2014}{article}{}
      \name{author}{2}{}{%
        {{hash=f526c15d2b091d43907bf2d50fcd1f16}{%
           family={Masoudnia},
           familyi={M\bibinitperiod},
           given={Saeed},
           giveni={S\bibinitperiod}}}%
        {{hash=1a58fd719dd0bb104006f663dbcb5ec4}{%
           family={Ebrahimpour},
           familyi={E\bibinitperiod},
           given={Reza},
           giveni={R\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en}%
      }
      \strng{namehash}{3782a1c02ac5311aed02c1261e822f0e}
      \strng{fullhash}{3782a1c02ac5311aed02c1261e822f0e}
      \strng{bibnamehash}{3782a1c02ac5311aed02c1261e822f0e}
      \strng{authorbibnamehash}{3782a1c02ac5311aed02c1261e822f0e}
      \strng{authornamehash}{3782a1c02ac5311aed02c1261e822f0e}
      \strng{authorfullhash}{3782a1c02ac5311aed02c1261e822f0e}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Mixture of experts (ME) is one of the most popular and interesting combining methods, which has great potential to improve performance in machine learning. ME is established based on the divide-and-conquer principle in which the problem space is divided between a few neural network experts, supervised by a gating network. In earlier works on ME, different strategies were developed to divide the problem space between the experts. To survey and analyse these methods more clearly, we present a categorisation of the ME literature based on this difference. Various ME implementations were classified into two groups, according to the partitioning strategies used and both how and when the gating network is involved in the partitioning and combining procedures. In the first group, The conventional ME and the extensions of this method stochastically partition the problem space into a number of subspaces using a special employed error function, and experts become specialised in each subspace. In the second group, the problem space is explicitly partitioned by the clustering method before the experts’ training process starts, and each expert is then assigned to one of these sub-spaces. Based on the implicit problem space partitioning using a tacit competitive process between the experts, we call the first group the mixture of implicitly localised experts (MILE), and the second group is called mixture of explicitly localised experts (MELE), as it uses pre-specified clusters. The properties of both groups are investigated in comparison with each other. Investigation of MILE versus MELE, discussing the advantages and disadvantages of each group, showed that the two approaches have complementary features. Moreover, the features of the ME method are compared with other popular combining methods, including boosting and negative correlation learning methods. As the investigated methods have complementary strengths and limitations, previous researches that attempted to combine their features in integrated approaches are reviewed and, moreover, some suggestions are proposed for future research directions.}
      \field{issn}{1573-7462}
      \field{journaltitle}{Artificial Intelligence Review}
      \field{month}{8}
      \field{number}{2}
      \field{shorttitle}{Mixture of experts}
      \field{title}{Mixture of experts: a literature survey}
      \field{urlday}{28}
      \field{urlmonth}{11}
      \field{urlyear}{2024}
      \field{volume}{42}
      \field{year}{2014}
      \field{urldateera}{ce}
      \field{pages}{275\bibrangedash 293}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1007/s10462-012-9338-y
      \endverb
      \verb{file}
      \verb Full Text PDF:files/1084/Masoudnia and Ebrahimpour - 2014 - Mixture of experts a literature survey.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/s10462-012-9338-y
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/s10462-012-9338-y
      \endverb
      \keyw{Artificial Intelligence,Classifier combining,Mixture of experts,Mixture of explicitly localised expert,Mixture of implicitly localised experts}
    \endentry
    \entry{Google_DFL}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=2ad70c06b0c3c9f658e668b458d5aa7c}{%
           family={Mayte},
           familyi={M\bibinitperiod},
           given={Aditya},
           giveni={A\bibinitperiod}}}%
        {{hash=6ece84fd17d0b3cac8ce602046aa0062}{%
           family={Taneja},
           familyi={T\bibinitperiod},
           given={Aparna},
           giveni={A\bibinitperiod}}}%
        {{hash=b7567123ad8063ff975519a285490851}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=f5f29a122eac8b3ac5ace411f219877a}{%
           family={Tambe},
           familyi={T\bibinitperiod},
           given={Milind\bibnamedelima Shashikant},
           giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=5474365d58798a592c399c3395f1ab0b}{%
           family={Verma},
           familyi={V\bibinitperiod},
           given={Shresth},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{85e2569e0c4c13a2900cd7ff1171a914}
      \strng{fullhash}{0f530925d5ee7b2d1c1e381baa58fdb5}
      \strng{bibnamehash}{85e2569e0c4c13a2900cd7ff1171a914}
      \strng{authorbibnamehash}{85e2569e0c4c13a2900cd7ff1171a914}
      \strng{authornamehash}{85e2569e0c4c13a2900cd7ff1171a914}
      \strng{authorfullhash}{0f530925d5ee7b2d1c1e381baa58fdb5}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Case Study: Applying Decision Focused Learning in the Real World}
      \field{year}{2022}
    \endentry
    \entry{bias}{article}{}
      \name{author}{5}{}{%
        {{hash=e49a830517defa430474d9c2b5c22d83}{%
           family={Mehrabi},
           familyi={M\bibinitperiod},
           given={Ninareh},
           giveni={N\bibinitperiod}}}%
        {{hash=6fc2e21e9c786462d54f7e2eeae61c19}{%
           family={Morstatter},
           familyi={M\bibinitperiod},
           given={Fred},
           giveni={F\bibinitperiod}}}%
        {{hash=125158a659e4885af8901c5155a4e57a}{%
           family={Saxena},
           familyi={S\bibinitperiod},
           given={Nripsuta},
           giveni={N\bibinitperiod}}}%
        {{hash=8aea0a421db201a447f5fec22e73a92f}{%
           family={Lerman},
           familyi={L\bibinitperiod},
           given={Kristina},
           giveni={K\bibinitperiod}}}%
        {{hash=73b781905368dfe2cc66cef5e33e0d32}{%
           family={Galstyan},
           familyi={G\bibinitperiod},
           given={Aram},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY, USA}%
      }
      \list{publisher}{1}{%
        {Association for Computing Machinery}%
      }
      \strng{namehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{fullhash}{70df8b815290d4668300a842f0dfe8bc}
      \strng{bibnamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authorbibnamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authornamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authorfullhash}{70df8b815290d4668300a842f0dfe8bc}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.}
      \field{issn}{0360-0300}
      \field{journaltitle}{ACM Comput. Surv.}
      \field{month}{7}
      \field{number}{6}
      \field{title}{A Survey on Bias and Fairness in Machine Learning}
      \field{volume}{54}
      \field{year}{2021}
      \verb{doi}
      \verb 10.1145/3457607
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1145/3457607
      \endverb
      \verb{url}
      \verb https://doi.org/10.1145/3457607
      \endverb
      \keyw{Fairness and bias in artificial intelligence,deep learning,machine learning,natural language processing,representation learning}
    \endentry
    \entry{rumelhart1986learning}{article}{}
      \name{author}{3}{}{%
        {{hash=55cd5380bb5b15032a6d5a2015f56e3f}{%
           family={Rumelhart},
           familyi={R\bibinitperiod},
           given={David\bibnamedelima E},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=6cbc29ad7fd57ffdb9ed4728418fd988}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Ronald\bibnamedelima J},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer Nature}%
      }
      \strng{namehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{fullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{bibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorbibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authornamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorfullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Nature}
      \field{number}{6088}
      \field{title}{Learning representations by back-propagating errors}
      \field{volume}{323}
      \field{year}{1986}
      \field{pages}{533\bibrangedash 536}
      \range{pages}{4}
      \verb{doi}
      \verb 10.1038/323533a0
      \endverb
      \verb{urlraw}
      \verb https://www.nature.com/articles/323533a0
      \endverb
      \verb{url}
      \verb https://www.nature.com/articles/323533a0
      \endverb
    \endentry
    \entry{P.Whittle}{article}{}
      \name{author}{1}{}{%
        {{hash=a793989addaf176e8d0343c358d72c1d}{%
           family={Whittle},
           familyi={W\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Applied Probability Trust}%
      }
      \strng{namehash}{a793989addaf176e8d0343c358d72c1d}
      \strng{fullhash}{a793989addaf176e8d0343c358d72c1d}
      \strng{bibnamehash}{a793989addaf176e8d0343c358d72c1d}
      \strng{authorbibnamehash}{a793989addaf176e8d0343c358d72c1d}
      \strng{authornamehash}{a793989addaf176e8d0343c358d72c1d}
      \strng{authorfullhash}{a793989addaf176e8d0343c358d72c1d}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider a population of n projects which in general continue to evolve whether in operation or not (although by different rules). It is desired to choose the projects in operation at each instant of time so as to maximise the expected rate of reward, under a constraint upon the expected number of projects in operation. The Lagrange multiplier associated with this constraint defines an index which reduces to the Gittins index when projects not being operated are static. If one is constrained to operate m projects exactly then arguments are advanced to support the conjecture that, for m and n large in constant ratio, the policy of operating the m projects of largest current index is nearly optimal. The index is evaluated for some particular projects.}
      \field{issn}{00219002}
      \field{journaltitle}{Journal of Applied Probability}
      \field{title}{Restless Bandits: Activity Allocation in a Changing World}
      \field{urlday}{12}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{25}
      \field{year}{1988}
      \field{urldateera}{ce}
      \field{pages}{287\bibrangedash 298}
      \range{pages}{12}
      \verb{urlraw}
      \verb http://www.jstor.org/stable/3214163
      \endverb
      \verb{url}
      \verb http://www.jstor.org/stable/3214163
      \endverb
    \endentry
    \entry{test_functions}{misc}{}
      \name{author}{1}{}{%
        {{hash=b6aea1a416c89509a7df1cbb69249cb6}{%
           family={{Wikipedia contributors}},
           familyi={W\bibinitperiod}}}%
      }
      \strng{namehash}{b6aea1a416c89509a7df1cbb69249cb6}
      \strng{fullhash}{b6aea1a416c89509a7df1cbb69249cb6}
      \strng{bibnamehash}{b6aea1a416c89509a7df1cbb69249cb6}
      \strng{authorbibnamehash}{b6aea1a416c89509a7df1cbb69249cb6}
      \strng{authornamehash}{b6aea1a416c89509a7df1cbb69249cb6}
      \strng{authorfullhash}{b6aea1a416c89509a7df1cbb69249cb6}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{note}{[Online; accessed 13-December-2024]}
      \field{title}{Test functions for optimization --- {Wikipedia}{,} The Free Encyclopedia}
      \field{year}{2024}
      \verb{urlraw}
      \verb https://en.wikipedia.org/w/index.php?title=Test_functions_for_optimization&oldid=1262242809
      \endverb
      \verb{url}
      \verb https://en.wikipedia.org/w/index.php?title=Test_functions_for_optimization&oldid=1262242809
      \endverb
    \endentry
    \entry{yu2024kanmlpfairercomparison}{misc}{}
      \name{author}{3}{}{%
        {{hash=c0c01a17a0f774c202eaba734ddfe060}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Runpeng},
           giveni={R\bibinitperiod}}}%
        {{hash=e8483440c1fc661bf3957f50ffb0df1d}{%
           family={Yu},
           familyi={Y\bibinitperiod},
           given={Weihao},
           giveni={W\bibinitperiod}}}%
        {{hash=c6099353a059c45f68d1f48dac694cf8}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Xinchao},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{7a469f97b843e6eac986c894bbaf510b}
      \strng{fullhash}{7a469f97b843e6eac986c894bbaf510b}
      \strng{bibnamehash}{7a469f97b843e6eac986c894bbaf510b}
      \strng{authorbibnamehash}{7a469f97b843e6eac986c894bbaf510b}
      \strng{authornamehash}{7a469f97b843e6eac986c894bbaf510b}
      \strng{authorfullhash}{7a469f97b843e6eac986c894bbaf510b}
      \field{sortinit}{Y}
      \field{sortinithash}{fd67ad5a9ef0f7456bdd9aab10fe1495}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{title}{KAN or MLP: A Fairer Comparison}
      \field{year}{2024}
      \verb{eprint}
      \verb 2407.16674
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/2407.16674
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/2407.16674
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

