\documentclass[12pt, letterpaper]{article}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{array}
\usepackage{longtable}
\usepackage{quotes}
\usepackage{amsmath}
\usepackage{hyperref}

% set reference files
\usepackage{biblatex}
\addbibresource{references.bib}

% set margins
\usepackage{geometry}
\geometry{margin=1in}

% Remove paragraph indentation
\setlength{\parindent}{0pt}


\title{Backpropagation through time for RNN}
\author{Qazi Zarif Ul Islam}

\onehalfspacing
\begin{document}
\maketitle

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.8\textwidth]{rnn.png}
    \caption*{Unfolded Recurrent Neural Net} (Credit: \cite{murat_bptt})
    \label{fig: rnn}
\end{figure}

This document is meant to be treated as supplementary to \cite{d2l_bptt} and assumes that the reader
understands the working principle of an RNN. If the reader requires such understanding, we recommend
reading the cited work first and only then reading this document. The aim of this document
is thus not to provide explanation on the RNN but rather to focus on one mathematical aspect
the cited work has failed to explain with sufficient readability and that is- The gradient of the loss of RNN
w.r.t. the \textit{hidden weights} $W_{hh}$. In truth, we shall
derive the gradient aka differentiate the loss at a particular
time instant wrt \textit{the concatenated weight matrix $[W_{xh}\;W_{hh}]^T$}
and ultimately obtain an equation for a particularly elusive-to-understand term in
the gradient (there are 3 terms in the gradient, this is one of them), the $\frac{\partial h_t}{\partial w_h}$
The concatenation of $W_{xh}$ and $W_{hh}$ does not make any difference from a computational
perspective. \cite{d2l_bptt, murat_bptt} and so, we shall denote it as $w_h$. Lastly, no distinction is 
made in terms of notation for vectors, scalars or matrices. This simplifies the derivation process however,
we shall produce a work in the future that incorporates vector-matrix notation.

\textbf{Let's start} 

The emprical loss of the neural network is,

\begin{equation}
    \hat{L} = \frac{1}{T} \sum_{t=1}^{T} \ell(y_t, d_t) = \frac{1}{T} (l_1 + l_2 + ... + l_t ... + l_T)
\end{equation}

For an RNN, the system is defined by,
\begin{align}
    h_{t} &= f (X_{t}, h_{t-1}) = \phi_{h}(W_{xh} \cdot X_{t} + W_{hh}\cdot h_{t-1} +b_{h}) \\
    \hat{o}_{t} &= f_{o}(h_{t}) = \phi_{o}(W_{hy}\cdot h_{t} + b_{y})
\end{align}

Let $w_h = [W_{xh}\;W_{hh}]^T$.

Thus,

\begin{equation}
    \frac{\partial \hat{L}}{\partial w_h} = \frac{1}{T} (\frac{\partial l_1}{\partial w_h} + \frac{\partial l_2}{\partial w_h} + ... + \frac{\partial l_t}{\partial w_h} ... + \frac{\partial l_T}{\partial w_h})
\end{equation}

Now the loss at a particular time instant t follows the chain rule of derivatives.

\begin{equation}
    \frac{\partial l_t}{\partial w_h} = \frac{\partial l_t}{\partial o_t}\frac{\partial o_t}{\partial h_t}\frac{\partial h_t}{\partial w_h}
    \label{eq: l_t}
\end{equation}

But $h_t$ is a function of $h_{t-1}$ and $w_h$ as well (besides $X_t$).
Furthermore, $h_{t-1}$ is again a function of $w_h$.
Thus, by the multivariable chain rule,

If $h_t = f_1(h_{t-1}, w_h), h_{t-1}= g_1(h_{t-2}, w_h), w_h = h_1(w_h)$ \footnote{(Note: $h_1$ (the function, \textit{not} the hidden state) is merely the identity
function. We've formulated it so just to be consistent with the
formulation of "case 1" in \cite{multivariate_chain_rule}. Eqn \ref{eq: h_t wrt wh} would still 
be valid even if we hadn't defined it as a separate function.} (Read as "The function $f_1$ takes 
$h_{t-1}$ and $w_h$ as input, $g_1$ takes $h_{t-2}$ and $w_h$ as input and $h_1$ takes $w_h$ as input.)

\begin{equation}
    \frac{\partial h_t}{\partial w_h} = \frac{\partial f_1}{\partial w_h} + \frac{\partial f_1}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial w_h}
    \label{eq: h_t wrt wh}
\end{equation}

Similarly, if $h_{t-1} = f_2(h_{t-2}, w_h), h_{t-2}= g_2(h_{t-3}, w_h), w_h = h_2(w_h)$

\begin{equation}
    \frac{\partial h_{t-1}}{\partial w_h} = \frac{\partial f_2}{\partial w_h} + \frac{\partial f_2}{\partial h_{t-2}}\frac{\partial h_{t-2}}{\partial w_h}
    \label{eq: h_t-1 wrt wh}
\end{equation}

Similarly, if $h_{t-2} = f_3(h_{t-3}, w_h), h_{t-3}= g_3(h_{t-4}, w_h), w_h = h_3(w_h)$

\begin{equation}
    \frac{\partial h_{t-2}}{\partial w_h} = \frac{\partial f_3}{\partial w_h} + \frac{\partial f_3}{\partial h_{t-3}}\frac{\partial h_{t-3}}{\partial w_h}
    \label{eq: h_t-2 wrt wh}
\end{equation}

We can find similar expressions for time steps further back $h_{t-3}, h_{t-4}$ all the way to $h_1$.

Now, from equation \ref{eq: h_t wrt wh} and equation \ref{eq: h_t-1 wrt wh}, 

\begin{align}
    \frac{\partial h_t}{\partial w_h} &= \frac{\partial f_1}{\partial w_h} + \frac{\partial f_1}{\partial h_{t-1}}(\frac{\partial f_2}{\partial w_h} + \frac{\partial f_2}{\partial h_{t-2}}\frac{\partial h_{t-2}}{\partial w_h}) && (\text{we won't expand $\frac{\partial h_{t-2}}{\partial w_h}$}) \\
                                      &= \frac{\partial f_1}{\partial w_h} + \frac{\partial f_1}{\partial h_{t-1}}\frac{\partial f_2}{\partial w_h} + \frac{\partial f_1}{\partial h_{t-1}}\frac{\partial f_2}{\partial h_{t-2}}\frac{\partial h_{t-2}}{\partial w_h} \\
                                      &= \frac{\partial h_t}{\partial w_h} + \frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial w_h} + \frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\frac{\partial h_{t-2}}{\partial w_h} && (\text{$f_1=h_t$, $f_2=h_{t-1}$})
\end{align}

which is equivalent to, 
\[
\boxed{\frac{\partial h_t}{\partial w_h} = \frac{\partial h_t}{\partial w_h} + \sum_{i=1}^{t-1}\prod_{j-1=i}^{t} (\frac{\partial h_j}{\partial h_{j-1}}) \frac{\partial h_i}{\partial w_h}
}
\]

Which is the same as equation 9.7.7 in \cite{d2l_bptt}.

% Now, we need to expand $\frac{\partial h_{t-2}}{\partial w_h}$ using eqn \ref{eq: h_t-2 wrt wh} to get the final
% form but we can instead write it more compactly by realizing that this equation is ultimately 
% a \textbf{summation of multiplications}. Let's break it down next.

\textbf{How do we arrive at the final form?:} In equation 11, if we expand $\frac{\partial h_{t-2}}{\partial w_h}$
(and subsequently expand more terms that recursively emerge), observe that every term's last (right-most) 
term is  partial of $h_i$ as $i$ \textbf{progresses} from the beginning and ends at $t$ as we move from right to left
in the summation. Hence, $i$ is our index of summation (The term that ``progresses" should be what we sum over).
Now, every term is again a cascade of products, decreasing in number of multipliers as we move from right to left in the summation.
Every multiplier term is a partial of $h_j$ wrt $h_{j-1}$. (Note that it is tempting to set our index of product as 
$i$, as it was for the summation, but doing this would not achieve the final form)


\printbibliography

\end{document}